### YamlMime:TSEnum
name: KnownTokenFilterNames
uid: '@azure/search-documents.KnownTokenFilterNames|beta'
package: '@azure/search-documents|beta'
summary: Defines values for TokenFilterName.
fullName: KnownTokenFilterNames
remarks: ''
isPreview: false
isDeprecated: false
fields:
  - name: Apostrophe
    uid: '@azure/search-documents.KnownTokenFilterNames.Apostrophe|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Strips all characters after an apostrophe (including the apostrophe
      itself). See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html
  - name: ArabicNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.ArabicNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      A token filter that applies the Arabic normalizer to normalize the
      orthography. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizationFilter.html
  - name: AsciiFolding
    uid: '@azure/search-documents.KnownTokenFilterNames.AsciiFolding|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Converts alphabetic, numeric, and symbolic Unicode characters which are
      not in the first 127

      ASCII characters (the "Basic Latin" Unicode block) into their ASCII
      equivalents, if such

      equivalents exist. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html
  - name: CjkBigram
    uid: '@azure/search-documents.KnownTokenFilterNames.CjkBigram|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Forms bigrams of CJK terms that are generated from StandardTokenizer. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html
  - name: CjkWidth
    uid: '@azure/search-documents.KnownTokenFilterNames.CjkWidth|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes CJK width differences. Folds fullwidth ASCII variants into the
      equivalent basic

      Latin, and half-width Katakana variants into the equivalent Kana. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html
  - name: Classic
    uid: '@azure/search-documents.KnownTokenFilterNames.Classic|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Removes English possessives, and dots from acronyms. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html
  - name: CommonGram
    uid: '@azure/search-documents.KnownTokenFilterNames.CommonGram|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Construct bigrams for frequently occurring terms while indexing. Single
      terms are still

      indexed too, with bigrams overlaid. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html
  - name: EdgeNGram
    uid: '@azure/search-documents.KnownTokenFilterNames.EdgeNGram|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Generates n-grams of the given size(s) starting from the front or the back
      of an input token.

      See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html
  - name: Elision
    uid: '@azure/search-documents.KnownTokenFilterNames.Elision|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Removes elisions. For example, "l'avion" (the plane) will be converted to
      "avion" (plane). See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html
  - name: GermanNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.GermanNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes German characters according to the heuristics of the German2
      snowball algorithm.

      See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html
  - name: HindiNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.HindiNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes text in Hindi to remove some differences in spelling
      variations. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizationFilter.html
  - name: IndicNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.IndicNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes the Unicode representation of text in Indian languages. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/in/IndicNormalizationFilter.html
  - name: KStem
    uid: '@azure/search-documents.KnownTokenFilterNames.KStem|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      A high-performance kstem filter for English. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html
  - name: KeywordRepeat
    uid: '@azure/search-documents.KnownTokenFilterNames.KeywordRepeat|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Emits each incoming token twice, once as keyword and once as non-keyword.
      See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordRepeatFilter.html
  - name: Length
    uid: '@azure/search-documents.KnownTokenFilterNames.Length|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Removes words that are too long or too short. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html
  - name: Limit
    uid: '@azure/search-documents.KnownTokenFilterNames.Limit|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Limits the number of tokens while indexing. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html
  - name: Lowercase
    uid: '@azure/search-documents.KnownTokenFilterNames.Lowercase|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes token text to lower case. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.htm
  - name: NGram
    uid: '@azure/search-documents.KnownTokenFilterNames.NGram|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Generates n-grams of the given size(s). See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html
  - name: PersianNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.PersianNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Applies normalization for Persian. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizationFilter.html
  - name: Phonetic
    uid: '@azure/search-documents.KnownTokenFilterNames.Phonetic|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Create tokens for phonetic matches. See

      https://lucene.apache.org/core/4_10_3/analyzers-phonetic/org/apache/lucene/analysis/phonetic/package-tree.html
  - name: PorterStem
    uid: '@azure/search-documents.KnownTokenFilterNames.PorterStem|beta'
    package: '@azure/search-documents|beta'
    summary: |-
      Uses the Porter stemming algorithm to transform the token stream. See
      http://tartarus.org/~martin/PorterStemmer
  - name: Reverse
    uid: '@azure/search-documents.KnownTokenFilterNames.Reverse|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Reverses the token string. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html
  - name: ScandinavianFoldingNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterNames.ScandinavianFoldingNormalization|beta
    package: '@azure/search-documents|beta'
    summary: >-
      Folds Scandinavian characters åÅäæÄÆ-&gt;a and öÖøØ-&gt;o. It also
      discriminates against use

      of double vowels aa, ae, ao, oe and oo, leaving just the first one. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html
  - name: ScandinavianNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterNames.ScandinavianNormalization|beta
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes use of the interchangeable Scandinavian characters. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html
  - name: Shingle
    uid: '@azure/search-documents.KnownTokenFilterNames.Shingle|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Creates combinations of tokens as a single token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html
  - name: Snowball
    uid: '@azure/search-documents.KnownTokenFilterNames.Snowball|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      A filter that stems words using a Snowball-generated stemmer. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html
  - name: SoraniNormalization
    uid: '@azure/search-documents.KnownTokenFilterNames.SoraniNormalization|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes the Unicode representation of Sorani text. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizationFilter.html
  - name: Stemmer
    uid: '@azure/search-documents.KnownTokenFilterNames.Stemmer|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Language specific stemming filter. See

      https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#TokenFilters
  - name: Stopwords
    uid: '@azure/search-documents.KnownTokenFilterNames.Stopwords|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Removes stop words from a token stream. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html
  - name: Trim
    uid: '@azure/search-documents.KnownTokenFilterNames.Trim|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Trims leading and trailing whitespace from tokens. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html
  - name: Truncate
    uid: '@azure/search-documents.KnownTokenFilterNames.Truncate|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Truncates the terms to a specific length. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html
  - name: Unique
    uid: '@azure/search-documents.KnownTokenFilterNames.Unique|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Filters out tokens with same text as the previous token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html
  - name: Uppercase
    uid: '@azure/search-documents.KnownTokenFilterNames.Uppercase|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Normalizes token text to upper case. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html
  - name: WordDelimiter
    uid: '@azure/search-documents.KnownTokenFilterNames.WordDelimiter|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      Splits words into subwords and performs optional transformations on
      subword groups.
