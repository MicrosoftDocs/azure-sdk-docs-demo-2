### YamlMime:TSType
name: KeywordTokenizer
uid: '@azure/search-documents.KeywordTokenizer|beta'
package: '@azure/search-documents|beta'
summary: >-
  Emits the entire input as a single token. This tokenizer is implemented using
  Apache Lucene.
fullName: KeywordTokenizer
remarks: ''
isPreview: false
isDeprecated: false
type: interface
properties:
  - name: maxTokenLength
    uid: '@azure/search-documents.KeywordTokenizer.maxTokenLength|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      The maximum token length. Default is 256. Tokens longer than the maximum
      length are split. The

      maximum token length that can be used is 300 characters. Default value:
      256.
    fullName: maxTokenLength
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'maxTokenLength?: number'
      return:
        description: ''
        type: number
  - name: name
    uid: '@azure/search-documents.KeywordTokenizer.name|beta'
    package: '@azure/search-documents|beta'
    summary: >-
      The name of the tokenizer. It must only contain letters, digits, spaces,
      dashes or

      underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters.
    fullName: name
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        description: ''
        type: string
  - name: odatatype
    uid: '@azure/search-documents.KeywordTokenizer.odatatype|beta'
    package: '@azure/search-documents|beta'
    summary: Polymorphic Discriminator
    fullName: odatatype
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        odatatype: "#Microsoft.Azure.Search.KeywordTokenizerV2" |
        "#Microsoft.Azure.Search.KeywordTokenizer"
      return:
        description: ''
        type: >-
          "#<xref uid="Microsoft.Azure.Search.KeywordTokenizerV2|beta" />" |
          "#<xref uid="Microsoft.Azure.Search.KeywordTokenizer|beta" />"
